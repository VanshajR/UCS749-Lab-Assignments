{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jar40U4OPeoS",
        "outputId": "7f401be5-cbd8-4fda-ffb7-9f631fd0ff69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this assignment is to develop an Automatic Speech Recognition (ASR) system using a public speech dataset and a deep learning-based model. You will preprocess the dataset, train a speech-to-text model, and evaluate its performance."
      ],
      "metadata": {
        "id": "JtN06d2Ov2Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch"
      ],
      "metadata": {
        "id": "8EdE7DvFv7CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained model and processor"
      ],
      "metadata": {
        "id": "_su8GWWuwCNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYh7Ex2qvx1C",
        "outputId": "839fe6d8-39c2-4c6c-a273-9bcad3aa8405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2ForCTC(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): Wav2Vec2GroupNormConvLayer(\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "        )\n",
              "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_projection): Wav2Vec2FeatureProjection(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): Wav2Vec2Encoder(\n",
              "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "        (conv): ParametrizedConv1d(\n",
              "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "          (parametrizations): ModuleDict(\n",
              "            (weight): ParametrizationList(\n",
              "              (0): _WeightNorm()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (padding): Wav2Vec2SamePadLayer()\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2SdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model_name = \"facebook/wav2vec2-large-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess an audio file"
      ],
      "metadata": {
        "id": "5AhqKvOPwIpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and process audio\n",
        "def load_audio(file_path, sample_rate):\n",
        "    waveform, sample_rate = torchaudio.load(file_path, sample_rate)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert stereo to mono if needed\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0)\n",
        "\n",
        "    return waveform.squeeze()"
      ],
      "metadata": {
        "id": "rr-zTT8jwJSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A48Opgy4Od_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert speech to text"
      ],
      "metadata": {
        "id": "BLKUCC24wN9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transcribe audio\n",
        "def transcribe(audio_file):\n",
        "    waveform = load_audio(audio_file,sample_rate)\n",
        "\n",
        "    # Ensure correct shape: [1, sequence_length]\n",
        "    waveform = waveform.unsqueeze(0)  # Add batch dimension -> Shape: [1, sequence_length]\n",
        "\n",
        "    # Process the input\n",
        "    input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Ensure correct shape before feeding into the model\n",
        "    input_values = input_values.squeeze(1)  # Shape: [batch_size, sequence_length]\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the predicted IDs\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "J-DmJmeywOd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(\"/content/drive/MyDrive/download.wav\")\n"
      ],
      "metadata": {
        "id": "88kwn8KBVfp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeZLaGg5WxaM",
        "outputId": "5341839e-d67c-47f7-bc63-b4cc70f2d770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform = torch.mean(waveform, dim=0)"
      ],
      "metadata": {
        "id": "gzfqP3NZVkYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform.unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgYZsvGfVvL_",
        "outputId": "6bdd8ee4-773a-490c-cdf7-c064e3fa03f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0183, 0.0180, 0.0180,  ..., 0.0018, 0.0019, 0.0032]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZhPJsh4Vyng",
        "outputId": "f0e43a1d-c4e2-4a6f-f8ec-00c07d25ee36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54400])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with an audio sample"
      ],
      "metadata": {
        "id": "AjTsbielwU0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = '/content/drive/MyDrive/download.wav'  # Replace with your test audio file\n",
        "text_output = transcribe(audio_file_path)\n",
        "print(\"Transcription:\", text_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eug9R0nLwYoT",
        "outputId": "25ff3199-d54e-45ec-f9d9-a08335a070ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: CURIOSITY BESIDE ME AT THIS MOMENT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from torchaudio.datasets import LIBRISPEECH\n",
        "test_dataset = LIBRISPEECH(root=\"./content/LibriSpeech/\", url=\"test-clean\", download=True)\n",
        "\n",
        "# Evaluate on a subset (e.g., first 20 samples)\n",
        "num_samples = 20\n",
        "ground_truths = []\n",
        "predictions = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    waveform, sample_rate, transcript, _, _, _ = test_dataset[i]\n",
        "\n",
        "    # Process and transcribe\n",
        "    waveform = load_audio(waveform, sample_rate)\n",
        "    predicted_text = transcribe(waveform)\n",
        "\n",
        "    # Store ground truth and predictions\n",
        "    ground_truths.append(transcript.lower())\n",
        "    predictions.append(predicted_text)\n",
        "\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Ground Truth: {transcript}\")\n",
        "    print(f\"Predicted: {predicted_text}\")\n",
        "\n",
        "# Compute Word Error Rate (WER)\n",
        "wer = wer_metric.compute(predictions=predictions, references=ground_truths)\n",
        "print(f\"Word Error Rate (WER): {wer:.2%}\")"
      ],
      "metadata": {
        "id": "wTuYj28TYTis"
      }
    }
  ]
}