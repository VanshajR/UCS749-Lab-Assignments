{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- Step 1: Feature Encoder (Temporal CNN) ----\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=512):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=10, stride=5, padding=2)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=8, stride=4, padding=2)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv1d(512, hidden_dim, kernel_size=4, stride=2, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.norm = nn.LayerNorm(hidden_dim)  # Layer Normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = x.permute(0, 2, 1)  # (B, T', C) for layer_norm\n",
        "        x = self.norm(x)\n",
        "        return x  # Feature embeddings\n",
        "\n",
        "# ---- Step 2: Context Network (Deeper Temporal CNN) ----\n",
        "class ContextNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, num_layers=9):\n",
        "        super(ContextNetwork, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Convert to (B, C, T')\n",
        "        for conv in self.convs:\n",
        "            x = self.relu(conv(x))\n",
        "        return x.permute(0, 2, 1)  # Back to (B, T', C)\n",
        "\n",
        "# ---- Step 3: Contrastive Loss ----\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z, z_pos, z_neg):\n",
        "        \"\"\"\n",
        "        z: Anchor representations (B, T', C)\n",
        "        z_pos: Positive representations (B, T', C)\n",
        "        z_neg: Negative samples (B, T', C)\n",
        "        \"\"\"\n",
        "        z = F.normalize(z, dim=-1)\n",
        "        z_pos = F.normalize(z_pos, dim=-1)\n",
        "        z_neg = F.normalize(z_neg, dim=-1)\n",
        "\n",
        "        # Compute similarity scores\n",
        "        pos_sim = (z * z_pos).sum(dim=-1) / self.temperature  # Positive similarity\n",
        "        neg_sim = (z * z_neg).sum(dim=-1) / self.temperature  # Negative similarity\n",
        "\n",
        "        # Contrastive loss: maximize difference between positive & negative\n",
        "        loss = -torch.mean(pos_sim - neg_sim)\n",
        "        return loss\n",
        "\n",
        "# ---- Step 4: Utility Function for Downsampling ----\n",
        "def downsample(x, feature_encoder):\n",
        "    \"\"\"Pass positive/negative samples through the same feature encoder to match time steps.\"\"\"\n",
        "    with torch.no_grad():  # No gradient needed for precomputed samples\n",
        "        x = feature_encoder(x)  # Now (B, T', C)\n",
        "    return x\n",
        "\n",
        "# ---- Step 5: Combine Everything into Wav2Vec 1.0 Model ----\n",
        "class Wav2Vec1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Wav2Vec1, self).__init__()\n",
        "        self.feature_encoder = FeatureEncoder()\n",
        "        self.context_network = ContextNetwork()\n",
        "        self.contrastive_loss = ContrastiveLoss()\n",
        "\n",
        "    def forward(self, x, z_pos, z_neg):\n",
        "        features = self.feature_encoder(x)   # (B, T', C)\n",
        "        z_pos = downsample(z_pos, self.feature_encoder)  # Match T'\n",
        "        z_neg = downsample(z_neg, self.feature_encoder)  # Match T'\n",
        "\n",
        "        context = self.context_network(features)  # (B, T', C)\n",
        "        loss = self.contrastive_loss(context, z_pos, z_neg)\n",
        "        return loss, context\n",
        "\n",
        "# ---- Step 6: Training on synthetic Data ----\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a random speech-like waveform (Batch size=4, Channels=1, Time=16000)\n",
        "    speech_waveform = torch.randn(4, 1, 16000)\n",
        "    positive_samples = torch.randn(4, 1, 16000)  # Augmented or real samples\n",
        "    negative_samples = torch.randn(4, 1, 16000)  # Distractor speech samples\n",
        "\n",
        "    model = Wav2Vec1()\n",
        "    loss, _ = model(speech_waveform, positive_samples, negative_samples)\n",
        "\n",
        "    print(f\"Contrastive Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7YA0TCJynO0",
        "outputId": "1b7be833-a5f1-4652-db81-1669aa0dddb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive Loss: -0.005177004262804985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "# Create the 'data' directory if it doesn't exist\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "# Now, download the dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"./data\", download=True)\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "\n",
        "waveform, sample_rate, label, *_ = dataset[0]\n",
        "print(f\"Waveform shape: {waveform.shape}, Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxXLd1c5t-Y4",
        "outputId": "553078fa-6e44-4e5b-d00c-081bf428437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:21<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 105829\n",
            "Waveform shape: torch.Size([1, 16000]), Label: backward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio.transforms as T\n",
        "def preprocess_audio(waveform, sample_rate, target_length=16000):\n",
        "    \"\"\"Preprocess the waveform: resample, trim, or pad.\"\"\"\n",
        "    # Convert stereo to mono if necessary\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Resample to 16kHz (if needed)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Trim or pad to target length\n",
        "    if waveform.shape[1] < target_length:\n",
        "        pad_amount = target_length - waveform.shape[1]\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
        "    else:\n",
        "        waveform = waveform[:, :target_length]\n",
        "\n",
        "    return waveform\n",
        "\n",
        "# Process the first sample\n",
        "speech_waveform = preprocess_audio(waveform, sample_rate)\n",
        "print(f\"Processed waveform shape: {speech_waveform.shape}\")  # Should be (1, 16000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0n1in-TurXi",
        "outputId": "878ca78a-6ab9-44d8-8acd-dfa4564ce8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed waveform shape: torch.Size([1, 16000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_random_sample(dataset, exclude_index=None):\n",
        "    \"\"\"Get a random sample from the dataset (ensuring it's different from the anchor).\"\"\"\n",
        "    while True:\n",
        "        idx = random.randint(0, len(dataset) - 1)\n",
        "        if idx != exclude_index:\n",
        "            waveform, sample_rate, _, _, _ = dataset[idx]\n",
        "            return preprocess_audio(waveform, sample_rate)\n",
        "\n",
        "# Positive sample (same speaker, slightly modified)\n",
        "positive_waveform = speech_waveform + 0.5 * torch.randn_like(speech_waveform)  # Adding noise\n",
        "\n",
        "# Negative sample (different speaker)\n",
        "negative_waveform = get_random_sample(dataset, exclude_index=0)\n",
        "\n",
        "# Stack for batch processing (batch size = 4)\n",
        "speech_waveform = speech_waveform.unsqueeze(0).repeat(4, 1, 1)  # (4, 1, 16000)\n",
        "positive_waveform = positive_waveform.unsqueeze(0).repeat(4, 1, 1)\n",
        "negative_waveform = negative_waveform.unsqueeze(0).repeat(4, 1, 1)\n",
        "\n",
        "print(f\"Final shapes - Speech: {speech_waveform.shape}, Positive: {positive_waveform.shape}, Negative: {negative_waveform.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbXzIaQsuwGG",
        "outputId": "59f9299a-7c15-41cd-ff2d-19712c39d2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final shapes - Speech: torch.Size([4, 1, 16000]), Positive: torch.Size([4, 1, 16000]), Negative: torch.Size([4, 1, 16000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Wav2Vec1 model\n",
        "model = Wav2Vec1()\n",
        "\n",
        "# Forward pass with real data\n",
        "loss, _ = model(speech_waveform, positive_waveform, negative_waveform)\n",
        "\n",
        "# Print loss value\n",
        "print(f\"Contrastive Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smApe9vGu30K",
        "outputId": "c15f80f2-921e-4cbe-da1c-15496a1727e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive Loss: -0.024169759824872017\n"
          ]
        }
      ]
    }
  ]
}